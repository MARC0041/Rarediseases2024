{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "# TODO: Using Memory search for now. Good idea to switch to vector database in future\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch, Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ollama(model=MODEL)\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading PDFs to create vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def get_pdf_paths(directory):\n",
    "  \"\"\"\n",
    "  This function recursively searches a directory and its subdirectories\n",
    "  for PDF files and returns a list of their full paths.\n",
    "  Args:\n",
    "      directory (str): The root directory to search.\n",
    "  Returns:\n",
    "      list: A list of full paths to all PDF files found.\n",
    "  \"\"\"\n",
    "  pdf_paths = []\n",
    "  for root, _, files in os.walk(directory):\n",
    "    for filename in files:\n",
    "      if filename.lower().endswith(\".pdf\"):  # Ensure case-insensitive matching\n",
    "        pdf_paths.append(os.path.join(root, filename))\n",
    "  return pdf_paths\n",
    "\n",
    "# Specify the folder path\n",
    "folder_path = r\"C:\\Users\\TechD\\Documents\\Marcus\\LLMs\\Rarediseases2024\\data\"\n",
    "pdfs = get_pdf_paths(folder_path)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_multiple_pdfs(pdf_paths):\n",
    "  \"\"\"\n",
    "  Loads multiple PDF files into LangChain for question answering.\n",
    "  Args:\n",
    "      pdf_paths (list): A list of full paths to the PDF files.\n",
    "  Returns:\n",
    "      langchain.RetrievalQA: A LangChain model for retrieving information\n",
    "          from the loaded PDFs.\n",
    "  \"\"\"\n",
    "\n",
    "  documents = []\n",
    "  embeddings = []\n",
    "\n",
    "  # Load each PDF and extract text\n",
    "  for path in pdf_paths:\n",
    "    loader = PyPDFLoader(path)\n",
    "    pages = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
    "\n",
    "    # for page in pages:\n",
    "    text = text_splitter.split_documents(pages)  # Split page text\n",
    "    documents.append(text)  # Store document text\n",
    "\n",
    "  # Create embeddings (replace with your preferred method)\n",
    "  embedding_model = OllamaEmbeddings(model=MODEL)\n",
    "  # embedding_model = OpenAIEmbeddings()  # Or other embedding model\n",
    "  # embeddings = embedding_model.create_embeddings(documents)\n",
    "  # vectorstore = Chroma.from_documents(documents=documents, embedding=embeddings)\n",
    "  vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embedding_model)\n",
    "  return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n",
    "vectorstore = load_multiple_pdfs(pdfs) # TODO: embeddings and model as parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Prompts and Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input template\n",
    "template = \"\"\"Answer the question based on the context below. If you can't answer the question, reply \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\"\"\"\n",
    "prompt = PromptTemplate.from_template(template = template)\n",
    "prompt.format(context=\"Here is some context\", question=\"Here is a question\")\n",
    "\n",
    "# Generate perspectives from question\n",
    "prompt_perspectives_template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = PromptTemplate.from_template(template = prompt_perspectives_template)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | model\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies for multiquery\n",
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "question = \"What is HPP?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is HPP?\n",
      "Answer: Based on the context, HPP stands for Hypophosphatasia, a genetic disorder characterized by impaired mineralization of bones and teeth.\n",
      "Question: Can you give me a summary of HPP?\n",
      "Answer: Based on the context provided, here is a summary of Hypophosphatasia (HPP):\n",
      "\n",
      "* HPP is a rare genetic disorder caused by mutations in either the ALP (Akp2) or PHOSPHO1 gene.\n",
      "* The main feature of HPP is impaired bone mineralization, leading to rickets and dental abnormalities.\n",
      "* Two key enzymes involved in HPP are TNAP (alkaline phosphatase) and NPP1 (nucleoside triphosphate pyrophosphorylase 1).\n",
      "* TNAP is essential for maintaining a proper concentration of phosphate (P) ions, which is necessary for bone mineralization.\n",
      "* In the absence of TNAP, PP levels accumulate in the extracellular space, leading to HPP symptoms.\n",
      "* NPP1 can act as a backup phosphatase in the absence of TNAP, but its activity is not sufficient to completely correct HPP symptoms.\n",
      "\n",
      "Overall, HPP is characterized by impaired mineralization of bones and teeth due to defects in the enzymes involved in phosphate regulation.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    # Input questions\n",
    "    \"What is HPP?\",\n",
    "    \"Can you give me a summary of HPP?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {final_rag_chain.invoke({'question':question})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
