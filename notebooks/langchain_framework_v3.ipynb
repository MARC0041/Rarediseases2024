{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sky appears blue to our eyes because of a phenomenon called scattering, which is the way that light interacts with tiny particles in the Earth's atmosphere.\n",
      "\n",
      "Here's what happens:\n",
      "\n",
      "1. **Sunlight**: The sun emits white light, which contains all the colors of the visible spectrum (red, orange, yellow, green, blue, indigo, and violet).\n",
      "2. **Atmosphere**: When this sunlight enters our atmosphere, it encounters tiny molecules of gases like nitrogen (N2) and oxygen (O2). These molecules are much smaller than the wavelength of light.\n",
      "3. **Scattering**: As the light interacts with these molecules, it gets scattered in all directions. This is because the molecules are much larger than the wavelength of light, so they can't absorb or reflect the light efficiently. Instead, they scatter the shorter wavelengths of light (like blue and violet) more than the longer wavelengths (like red and orange).\n",
      "4. **Blue dominance**: As a result of this scattering, the blue and violet parts of the sunlight are distributed evenly throughout the sky, making it appear blue to our eyes.\n",
      "5. **Reddening**: Meanwhile, the longer wavelengths of light (like red and orange) continue to travel in more direct paths to our eyes, arriving at slightly different times due to their longer distances traveled. This is why sunsets often have a reddish hue, as the shorter wavelengths of blue light are scattered away, leaving mainly the longer wavelengths of red and orange light to reach our eyes.\n",
      "\n",
      "So, to summarize: the sky appears blue because the shorter wavelengths of sunlight (like blue and violet) are scattered in all directions by tiny molecules in the atmosphere, while the longer wavelengths (like red and orange) continue to travel more directly to our eyes. This scattering effect gives us the beautiful blue color we associate with a clear sky!\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model='llama3', messages=[\n",
    "  {\n",
    "    'role': 'user',\n",
    "    'content': 'Why is the sky blue?',\n",
    "  },\n",
    "])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "\n",
    "# TODO: Using Memory search for now. Good idea to switch to vector database in future\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "model = Ollama(model=MODEL)\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input template\n",
    "template = \"\"\"Answer the question based on the context below. If you can't answer the question, reply \"I don't know\".\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer the question based on the context below. If you can\\'t answer the question, reply \"I don\\'t know\".\\nContext: Here is some context\\n\\nQuestion: Here is a question'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(template = template)\n",
    "prompt.format(context=\"Here is some context\", question=\"Here is a question\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For ben, please input the code to read out the PDFs / other documents here. If more refactoring is needed, we can just do for PDF first, and work on other file types later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read PDFs and create the embeddings\n",
    "# sample code for editing  - Ben\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "# \"C:\\Users\\TechD\\Documents\\Marcus\\LLMs\\Rarediseases2024\\data\\HPP Papers (PDF) Via Liezl Puzon\\10.1002___ajmg.a.33146.pdf\"\n",
    "loader = PyPDFLoader(os.path.join(\"data\", \"HPP Papers (PDF) Via Liezl Puzon\", \"10.1002___ajmg.a.33146.pdf\"))\n",
    "pages = loader.load_and_split()\n",
    "embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever, \n",
    "        \"question\":itemgetter(\"question\") \n",
    "\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is HPP?\n",
      "Answer: Based on the context, I don't know what HPP stands for, but it appears to be related to a medical condition or disorder, possibly Cleidocranial dysplasia, given the presence of radiographic images and genetic information.\n",
      "Question: Can you give me a summary of HPP?\n",
      "Answer: Based on the provided context, Hypophosphatasia (HPP) is described as a distinct skeletal disorder characterized by severe osteopenia, low alkaline phosphatase, and features such as Bowdler spurs. The text does not provide a detailed summary of HPP, but it mentions that the patient in question has clinical features which mimic hypophosphatasia.\n"
     ]
    }
   ],
   "source": [
    "questions = [\n",
    "    # Input questions\n",
    "    \"What is HPP?\",\n",
    "    \"Can you give me a summary of HPP?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {chain.invoke({'question':question})}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
