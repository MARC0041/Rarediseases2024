import os
from dotenv import load_dotenv
from langchain_community.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
from langchain.prompts import PromptTemplate
from operator import itemgetter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import DocArrayInMemorySearch,Chroma
# dependencies for multiquery
from langchain.load import dumps, loads
def get_unique_union(documents: list[list]):
    """ Unique union of retrieved docs """
    # Flatten list of lists, and convert each Document to string
    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]
    # Get unique documents
    unique_docs = list(set(flattened_docs))
    # Return
    return [loads(doc) for doc in unique_docs]

class LLMModel:
    def __init__(self):
        load_dotenv()
        self.model_name = os.getenv("MODEL", "llama3")
        self.model = Ollama(model=self.model_name)
        self.parser = StrOutputParser()

        # Define the prompt template
        self.template = """Answer the question based on the context below. If you can't answer the question, reply "I don't know".
Context: {context}

Question: {question}"""
        self.prompt = PromptTemplate.from_template(template=self.template)
        self.prompt.format(context="Here is some context", question="Here is a question")


    def load_document(self, path):
        """Loads a document from the specified path and extracts its content."""
        loader = PyPDFLoader(path)
        pages = loader.load_and_split()
        return pages

    def create_embeddings(self):
        """Creates embeddings for the provided documents using the Ollama embeddings."""
        self.embeddings = OllamaEmbeddings(model=self.model_name)
        return

    def build_search(self, documents):
        """Builds a document search using DocArrayInMemorySearch."""
        embeddings = self.create_embeddings()
        vectorstore = DocArrayInMemorySearch.from_documents(
            documents, embedding=embeddings
        )
        self.retriever = vectorstore.as_retriever()
        return 
    def create_and_save_vector_store(self, docs, persist_directory):
        vectordb = Chroma.from_documents(docs, self.embeddings, persist_directory=persist_directory)
        # Persist the database to disk
        vectordb.persist()
        return vectordb
    def load_persisted_db(self, persist_directory):
        vectordb = Chroma(persist_directory=persist_directory)
        return vectordb
    def answer_question(self, question):
        """Answers a question based on the context, question, and provided documents.

        Args:
            context: The context to provide to the model.
            question: The question to be answered.
            documents: A list of documents to search for relevant information.

        Returns:
            The answer generated by the model.
        """

        # Input template
        template = """Answer the question based on the context below. If you can't answer the question, reply "I don't know".
        Context: {context}
        Question: {question}"""
        prompt = PromptTemplate.from_template(template = template)
        prompt.format(context="Here is some context", question="Here is a question")

        # Generate perspectives from question
        prompt_perspectives_template = """You are an AI language model assistant. Your task is to generate five 
        different versions of the given user question to retrieve relevant documents from a vector 
        database. By generating multiple perspectives on the user question, your goal is to help
        the user overcome some of the limitations of the distance-based similarity search. 
        Provide these alternative questions separated by newlines. Original question: {question}"""
        prompt_perspectives = PromptTemplate.from_template(template = prompt_perspectives_template)
        generate_queries = (
            prompt_perspectives 
            | self.model
            | StrOutputParser() 
            | (lambda x: x.split("\n"))
        )

        retrieval_chain = generate_queries | self.retriever.map() | get_unique_union
        final_rag_chain = (
            {"context": retrieval_chain, 
            "question": itemgetter("question")} 
            | prompt
            | self.model
            | StrOutputParser()
        )
        return final_rag_chain.invoke({'question':question})
    

from langchain.document_loaders import DirectoryLoader, TextLoader

if __name__ == "__main__":
    # Example usage
    model = LLMModel()

    pdf_loader = DirectoryLoader("data/", glob="*/*.pdf", loader_cls=PyPDFLoader,silent_errors=True)
    pdf_documents = pdf_loader.load()
    txt_loader = DirectoryLoader("data/", glob="*/*.txt", loader_cls=TextLoader,silent_errors=True)
    txt_documents = txt_loader.load()
    partial_docs = [*pdf_documents, *txt_documents]
    model.build_search(partial_docs)

    questions = [
    # Input questions
    "What is HPP?",
    "Can you give me a summary of HPP?"
    ]
    for q in questions:
        answer = model.answer_question(q)
        print(f"Question: {q}")
        print(f"Answer: {answer}")
